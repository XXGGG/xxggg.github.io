---
title: 🕷️『爬虫第零步』🕷️
---

## 需要了解的知识点
首先爬虫的第一步就是需要了解到，爬虫需要知道的哪些知识点！
1. 目前爬虫使用的是【Python】
2. 需要使用到很多【Python】第三方库【urllib】【BeautifulSoup】...
3. 需要会数据库语句【SQlite】或【MySql】


## Python爬虫基本流程
### 1. 准备工作
通过浏览器查看分析目标网页，学习编程基础规范
### 2. 获取数据
通过HTTP库向目标站点发起请求，请求可以包含额外的header等信息  
如果服务器能正常响应，会得到一个Response，便是所要获得的页面内容。
### 3. 解析内容
得到的内容可能是HTML、json等，可以用**页面解析库**、**正则表达式**等进行解析
### 4. 保存数据
保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件。

--- 
## 编码规范
### 1.一般Python程序第一行需要加入
```py
# -*- coding:uft-8 -*- 
或者
# coding=utf-8
```

### 2. Python文件中可以加入main函数用于测试程序

```py
#可以把他当作是整个程序的开始！
if __name__ == "__main__":
```
---

## 常用第三方库👇
- urllib （1.制定URL、获取“整体”网页数据）【Python3自带】
- bs4、BeautifulSoup （2.网页解析、变成“可筛选”数据）
- re （3.正则表达式，进行筛选，整理出想要的内容）【Python3自带】
- xlwt （4.进行excel操作，保存进excel文档里）
- sqlite3 （4.进行SQL数据库操作）【Python3自带】

安装方法：👇
```sh
# python3自带的不用安装
pip install bs4
pip install xlwt
```
引入：👇  
```py
import urllib.request,urllib.error
from bs4 import BeautifulSoup
import re
import xwlt
import sqlite3
```

## 【urllib】的使用
- urllib.request.Request() 来设置请求头
- urllib.request.urlopen() 来请求网页数据
```py
# 开始爬取数据
def askURL(baseurl):
    #在爬虫的时候 这个是个简单的设置请求头的方法
    head = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.54"
        }
    request = urllib.request.Request(baseurl,headers=head)
    #因为如果你不设置，他们服务器是会知道你是python爬虫来拿数据的。所以要伪装一下
    html = ''
    #爬虫一定要使用这个try 因为怕程序出错 爬不出来
    try:
        #然后利用【urllib里的request的urlopen方法】根据上面的请求头和url爬出来
        response = urllib.request.urlopen(request)
        #把爬到的内容【对象】赋值给response变量后读出来-用utf-8的格式读出来
        html = response.read().decode('utf-8')
    
    #出错了就用这个报错
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)

    #最后把爬到的数据返回出去
    return html
```

## 来自【bs4】里的【BeautifuSoup】

1. `from bs4 import BeautifulSoup` 【引入】
2. `bs = BeautifulSoup(第一个参数是爬取回来的网页的变量名，"html.parser")` 【解析】
3. `bs.find_all("a")` 【筛选】 查找所有a标签 

```py
# bs解析 re筛选 整理数据
def getData(baseurl):
    datalist = []
    html = askURL(baseurl) #在这里调用爬取 并取回数据
    soup = BeautifulSoup(html,"html.parser") #解析数据
    #来到这就代表 已经爬取到了信息
    
    #下面这是把爬取到的内容进行筛选 （利用re正则）
    for item in soup.find_all('div',class_="placeholder one-img-plc"):
        #用find_all找出一个带有特定class的div，他们会形成一个【数组】
        # 再把这个数组遍历出来 成- item
        data = []
        item = str(item) #把item变成字符串 好用re正则来查找

        #下面的finkLink是 事先制定好的正则规则 而后面的item就是每条数据的字符串了
        link = re.findall(findLink,item)[0]
        #同样的他会返回一个数据 于是为了方便就使用[0] 来直接得到数组第一个数据
        imgSrc = re.findall(findImgSrc,item)[0]
        title = re.findall(findTitle,item)[0]
        #经过网页观察 没时间的就是广告
        time = re.findall(findTime,item)[0]

        if bool(time):
            data.append(link)
            data.append(imgSrc)
            data.append(title)
            data.append(time)
            #把数据整理好 添加到data 这个变量来
            #最后再把data这一条数据 整体的增加进datalist
            datalist.append(data)
        else: 
            print("这个是广告")

    # 最后把这个datalist再返回出去 
    return datalist
```
---

**那么我们进行了urllib的爬取！也进行了BeautifulSoup的解析 跟re的筛选，最后整理好了数据数组列表datalist 返回出去返回到了哪里呢？**

```py
if __name__ == "__main__":
    main()
    print('爬取完成！')

def main():
    baseurl = "https://m.ithome.com/"
    datalist = getData(baseurl)
    #👇下面是为了保存爬取到的内容
    savepath = ('.\\IT之家最新新闻.xls')
    saveData(datalist,savepath)
```
在这里 main 主程序入口 这里  
1. 先定义好 基础的链接  
2. 然后调用 getData 这个自定义的函数取爬取 datalist就会返回到这里来
3. 后面两步是为了保存我们爬取到的内容

---

### 正则表达式 搜索 - bs.find_all

- 使用`search()`方法来匹配内容
```py
bs.find_all(re.compile("a"))
#找到有带a的东西
```
```py
bs.find_all(id="head")
#找到id为head的内容
```

## css选择器 查找 select
```py
bs.select('title')
#查找到这个标签
```
```py
bs.select(".class")
#查找到这个类 返回一个列表
bs.select("#id")
#查找到这个id 返回一个列表
bs.select("a[class='nav']")
#查找到这个class是nav的a标签 返回一个列表
```

## re 正则表达式
```py
re.findall('正则表达式','被验证的语句、字符串')
re.findall('/d','123abc')
```
```py
re.sub('要替换的正则表达式','替换的','被替换的字符串')
re.sub('abc','Abc','abcdefg')
#sub替换
```







## 报错
- 418 代表被发现是爬虫

---


## 其他

### 转换成二进制
```
bytes()
```
### 百度搜索指数
[index.baidu.com](index.baidu.com)