---
title: 🕷️『爬虫第一步』🕷️
---


## 爬虫第一步

### 1.开头
开头写
```py
# coding=utf-8
```
### 2.引入模块
```py
from bs4 import BeautifulSoup
import urllib.request,urllib.error
import re
import xlwt
import sqlite3
```

### 3.分工合作
定义好各个函数的功能，分工合作

```py
#前面写1.开头 2.引入模块... 

if __name__ == "__main__":
    main()
    print("爬取并保存完毕！")

# 1.主程序 入口 开始！
def main():
    baseurl = "https://m.ithome.com/" 
    datalist = getData(baseurl)
    savepath = ('.\\IT之家最新新闻.xls')
    saveData(datalist,savepath)


# 2. 【re-正则表达式】 （筛选规则）
findLink = re.compile(r'<a href="(.*?)" role="option" target="_blank">')
#...

# 3. 【bs解析 re筛选】 整理数据
def getData(baseurl):
    datalist = []
    html = askURL(baseurl)
    soup = BeautifulSoup(html,"html.parser")
    #来到这就代表 已经爬取到了信息
    
    #下面这是把爬取到的内容进行筛选 （利用正则）
    for item in soup.find_all('div',class_="placeholder one-img-plc"):
        data = []
        item = str(item)

        link = re.findall(findLink,item)[0]
        #爬取详情
        details = detailsURL(link)
        details_soup = BeautifulSoup(details,"html.parser")
        news = details_soup.find_all('div',"news-content")[0]
        news = str(news)
        # print(news)

        #新闻时间
        item_time = details_soup.find_all('span',"news-time")
        item_time = str(item_time)
        news_time = re.findall(find_news_time,item_time)
        if not bool(news_time):
            # print("这个是it之家收集别人的新闻")
            item_time = details_soup.find_all('span',"post-time")[0]
            item_time = str(item_time)
            news_time = re.findall(find_post_time,item_time)[0]
        else:
            news_time = re.findall(find_news_time,item_time)[0]
        # print(news_time)
        imgSrc = re.findall(findImgSrc,item)[0]

        title = re.findall(findTitle,item)[0]

        #经过网页观察 没时间的就是广告
        time = re.findall(findTime,item)[0]

        if bool(time):
            # print("这个不是广告")
            data.append(link)
            data.append(imgSrc)
            data.append(title)
            data.append(time)
            data.append(news_time)
            data.append(news)
            datalist.append(data)
        else: 
            print("这个是广告")

    print("返回爬取并筛选好的数据列表")
    return datalist

# 4.【爬】urllib 开始爬取数据
def askURL(baseurl):
    # 写请求头进行 伪装
    head = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.54"
    }
    request = urllib.request.Request(baseurl,headers=head)
    html = ''
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode('utf-8')
        # print(html)
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)

    return html

# 4.【爬】urllib 爬取详情链接里面的内容
def detailsURL(link):
    # 写请求头进行 伪装
    head = {
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.54"
    }
    request = urllib.request.Request(link,headers=head)
    details = ''
    try:
        response = urllib.request.urlopen(request)
        details = response.read().decode('utf-8')
    except urllib.error.URLError as e:
        if hasattr(e,"code"):
            print(e.code)
        if hasattr(e,"reason"):
            print(e.reason)

    return details

#5.【保存】把爬取到 并且筛选好的数据 保存起来
def saveData(datalist,savepath):
    print('开始保存...')
    book = xlwt.Workbook(encoding='utf-8')
    sheet = book.add_sheet('IT新闻',cell_overwrite_ok=True)
    col = ("原链接","图片链接","新闻标题","时间","详情时间","内容html")
    for i in range(0,6):
        sheet.write(0,i,col[i])
    for i in range(len(datalist)):
        print("第%d条" %(i+1))
        data = datalist[i]
        for j in range(0,6):
            sheet.write(i+1,j,data[j])
    
    book.save(savepath)
```


